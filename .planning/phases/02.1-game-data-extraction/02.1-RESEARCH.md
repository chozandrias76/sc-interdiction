# Phase 2.1: Game Data Extraction Pipeline - Research

**Researched:** 2026-01-16
**Domain:** Star Citizen Data.p4k extraction and DataForge DCB parsing
**Confidence:** HIGH

<research_summary>
## Summary

Researched the Star Citizen game data extraction ecosystem for building a pipeline to extract authoritative item/contract data from `Data.p4k`. The standard approach uses existing community tools (unp4k, scdatatools, StarBreaker) rather than native parsing—these formats are complex, reverse-engineered, and change with game updates.

**Key finding:** No Rust crate exists for p4k/dcb parsing. All mature tools are C# (.NET) or Python. For a project focused on interdiction intel (not game modding), the pragmatic approach is to shell out to existing tools for extraction, then consume the XML/JSON output natively in Rust.

**Primary recommendation:** Use `scdatatools` (Python) for extraction—it's actively maintained (July 2025), has a clean CLI, and outputs JSON. Shell out from Rust, parse the JSON output. Don't hand-roll format parsers.
</research_summary>

<standard_stack>
## Standard Stack

### Core (Extraction Tools - External)
| Tool | Language | Purpose | Why Standard |
|------|----------|---------|--------------|
| scdatatools | Python | P4k extraction + DCB to JSON | Actively maintained, clean API, JSON output |
| unp4k/unforge | C# (.NET) | P4k extraction + DCB to XML | Most mature, widely used by community |
| 7z | Native | Basic p4k listing/extraction | Already installed, handles ZSTD |

### Core (Rust Consumption)
| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| serde_json | workspace | Parse extracted JSON | Already in workspace |
| std::process::Command | std | Shell out to extraction tools | Standard approach for external tools |
| tempfile | 3.x | Temp dirs for extraction output | Clean handling of intermediate files |

### Supporting
| Library | Version | Purpose | When to Use |
|---------|---------|---------|-------------|
| which | 6.x | Find installed tools | Verify scdatatools/7z available |
| tracing | workspace | Logging extraction progress | Already in workspace |

### Alternatives Considered
| Instead of | Could Use | Tradeoff |
|------------|-----------|----------|
| scdatatools | unp4k/unforge | unp4k outputs XML (larger), requires .NET on Linux |
| scdatatools | StarBreaker | StarBreaker is C#, more complex setup |
| Shell out | Native Rust parser | Massive effort, format changes with updates, not our core value |
| JSON output | XML output | JSON is smaller, easier to parse with serde |

**Installation:**
```bash
# Python scdatatools (recommended)
pip install scdatatools

# Or via pipx for isolated install
pipx install scdatatools

# Verify installation
scdt --help
```
</standard_stack>

<architecture_patterns>
## Architecture Patterns

### Recommended Project Structure
```
crates/sc-data-extractor/
├── Cargo.toml
└── src/
    ├── lib.rs           # Re-exports
    ├── extractor.rs     # DataExtractor struct (shells out to tools)
    ├── p4k.rs           # P4k-specific operations
    ├── dataforge.rs     # DCB parsing (from JSON output)
    ├── localization.rs  # global.ini parsing
    └── error.rs         # Error types
```

### Pattern 1: Tool Abstraction Layer
**What:** Abstract over external tools so implementation can change
**When to use:** When shelling out to external processes
**Example:**
```rust
pub trait DataExtractor {
    /// Extract specific files from Data.p4k
    fn extract_files(&self, patterns: &[&str], output_dir: &Path) -> Result<Vec<PathBuf>>;

    /// Convert DCB to JSON
    fn dcb_to_json(&self, dcb_path: &Path, output_path: &Path) -> Result<()>;
}

pub struct ScdatatoolsExtractor {
    scdt_path: PathBuf,  // Path to scdt CLI
}

impl DataExtractor for ScdatatoolsExtractor {
    fn extract_files(&self, patterns: &[&str], output_dir: &Path) -> Result<Vec<PathBuf>> {
        // scdt unp4k extract --filter "pattern" --output dir
        let output = Command::new(&self.scdt_path)
            .args(["unp4k", "extract"])
            .arg("--filter").arg(patterns.join(","))
            .arg("--output").arg(output_dir)
            .arg(&self.p4k_path)
            .output()?;
        // ...
    }
}
```

### Pattern 2: Lazy Extraction with Caching
**What:** Extract once, cache results, re-extract only when p4k changes
**When to use:** Extraction is slow (150GB archive)
**Example:**
```rust
pub struct CachedExtractor {
    inner: Box<dyn DataExtractor>,
    cache_dir: PathBuf,
    p4k_hash: Option<String>,  // Hash of Data.p4k modification time + size
}

impl CachedExtractor {
    pub fn get_dataforge(&self) -> Result<DataForge> {
        let cache_path = self.cache_dir.join("dataforge.json");

        if cache_path.exists() && self.is_cache_valid()? {
            // Load from cache
            let json = std::fs::read_to_string(&cache_path)?;
            return Ok(serde_json::from_str(&json)?);
        }

        // Extract fresh
        self.inner.dcb_to_json(...)?;
        // ...
    }
}
```

### Pattern 3: Typed DataForge Records
**What:** Define Rust structs for specific record types we care about
**When to use:** When consuming specific data from DataForge
**Example:**
```rust
// Only define types for records we actually need
#[derive(Debug, Deserialize)]
pub struct MissionContract {
    #[serde(rename = "__rec")]
    pub record_name: String,
    pub title: LocalizationKey,
    pub requirements: Vec<ContractRequirement>,
    // ...
}

#[derive(Debug, Deserialize)]
pub struct ContractRequirement {
    pub item_type: String,
    pub quantity: u32,
    // ...
}

// Parse only Wikelo-related records from the full DataForge dump
pub fn find_wikelo_contracts(dataforge_json: &Path) -> Result<Vec<MissionContract>> {
    // Stream through JSON, filter to Wikelo contracts
}
```

### Anti-Patterns to Avoid
- **Native p4k/dcb parsing:** Complex reverse-engineered formats, change with updates, not our core value
- **Extracting everything:** Data.p4k is 150GB; extract only what's needed
- **Blocking extraction on startup:** Extract async/on-demand, cache results
- **Defining types for all DataForge records:** Only type what we consume
</architecture_patterns>

<dont_hand_roll>
## Don't Hand-Roll

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| P4k extraction | Native Rust p4k reader | scdatatools/7z | Encrypted ZIP with ZSTD + CryEngine encryption; community tools handle updates |
| DCB parsing | Native Rust dcb parser | scdatatools unforge | Complex binary format, changes with game versions |
| CryXML parsing | Custom XML parser | scdatatools cryxml-to-json | Proprietary serialized XML format |
| Localization lookup | Manual string parsing | Parse global.ini once | Simple INI format, but 9MB of strings |

**Key insight:** This project's value is interdiction intel, not game file reverse engineering. The SC modding community maintains extraction tools that track game updates. Using their tools means we benefit from their maintenance without taking on that burden. A native Rust parser would be impressive but would require constant updates as CIG changes formats.
</dont_hand_roll>

<common_pitfalls>
## Common Pitfalls

### Pitfall 1: Extraction Performance
**What goes wrong:** Extraction takes minutes, blocks application startup
**Why it happens:** Data.p4k is 150GB, even selective extraction is slow
**How to avoid:** Extract once at setup time, cache to disk, validate cache on startup
**Warning signs:** Long startup times, users complaining about "loading"

### Pitfall 2: Tool Not Installed
**What goes wrong:** Shell out fails because scdatatools/7z not in PATH
**Why it happens:** Assuming tools are installed
**How to avoid:** Check for tools at startup, provide clear error with install instructions
**Warning signs:** "command not found" errors at runtime

### Pitfall 3: Format Version Mismatch
**What goes wrong:** Parser fails on new game version
**Why it happens:** CIG updates DataForge format, our cached data is stale
**How to avoid:** Track game version alongside cache; re-extract on update
**Warning signs:** Deserialization errors after game patch

### Pitfall 4: Memory Explosion
**What goes wrong:** OOM when loading full DataForge JSON
**Why it happens:** Game2.dcb converts to ~1-2GB JSON
**How to avoid:** Stream parse with serde, filter to needed records only
**Warning signs:** High memory usage during data load

### Pitfall 5: Windows Path Handling
**What goes wrong:** Paths with spaces fail when shelling out
**Why it happens:** `/mnt/c/Star Citizen/...` has spaces
**How to avoid:** Always quote paths, use OsString properly
**Warning signs:** "file not found" for paths that exist
</common_pitfalls>

<code_examples>
## Code Examples

### Check for Required Tools
```rust
// Source: common pattern for external tool dependencies
use which::which;

pub fn check_dependencies() -> Result<(), Vec<String>> {
    let mut missing = Vec::new();

    if which("scdt").is_err() {
        missing.push("scdatatools not found. Install with: pip install scdatatools".into());
    }

    if which("7z").is_err() {
        missing.push("7z not found. Install with: sudo apt install p7zip-full".into());
    }

    if missing.is_empty() {
        Ok(())
    } else {
        Err(missing)
    }
}
```

### Shell Out to scdatatools
```rust
// Source: std::process::Command patterns
use std::process::Command;

pub fn extract_dataforge(p4k_path: &Path, output_dir: &Path) -> Result<PathBuf> {
    let dcb_output = output_dir.join("Game2.json");

    // Extract Game2.dcb from p4k
    let status = Command::new("scdt")
        .args(["unp4k", "extract"])
        .arg("--filter").arg("Data/Game2.dcb")
        .arg("--output").arg(output_dir)
        .arg(p4k_path)
        .status()?;

    if !status.success() {
        return Err(anyhow!("Failed to extract Game2.dcb"));
    }

    // Convert dcb to json
    let dcb_path = output_dir.join("Data/Game2.dcb");
    let status = Command::new("scdt")
        .args(["unforge"])
        .arg(&dcb_path)
        .arg("--output").arg(&dcb_output)
        .arg("--format").arg("json")
        .status()?;

    if !status.success() {
        return Err(anyhow!("Failed to convert Game2.dcb to JSON"));
    }

    Ok(dcb_output)
}
```

### Parse Localization File
```rust
// Source: global.ini is simple key=value format
use std::collections::HashMap;

pub fn parse_localization(global_ini_path: &Path) -> Result<HashMap<String, String>> {
    let content = std::fs::read_to_string(global_ini_path)?;
    let mut map = HashMap::new();

    for line in content.lines() {
        // Skip BOM, comments, empty lines
        let line = line.trim_start_matches('\u{feff}').trim();
        if line.is_empty() || line.starts_with(';') || line.starts_with('#') {
            continue;
        }

        if let Some((key, value)) = line.split_once('=') {
            map.insert(key.to_string(), value.to_string());
        }
    }

    Ok(map)
}
```

### Stream Parse Large JSON
```rust
// Source: serde_json streaming for large files
use serde::Deserialize;
use std::io::BufReader;

#[derive(Deserialize)]
struct DataForgeRecord {
    #[serde(rename = "__rec")]
    record_name: String,
    #[serde(rename = "__type")]
    record_type: String,
    // ... other fields
}

pub fn find_records_by_type<P: AsRef<Path>>(
    json_path: P,
    type_filter: &str,
) -> Result<Vec<DataForgeRecord>> {
    // For very large files, consider using serde_json::StreamDeserializer
    // or jq for pre-filtering
    let file = std::fs::File::open(json_path)?;
    let reader = BufReader::new(file);

    // If the JSON is an array of records:
    let records: Vec<DataForgeRecord> = serde_json::from_reader(reader)?;

    Ok(records
        .into_iter()
        .filter(|r| r.record_type.contains(type_filter))
        .collect())
}
```
</code_examples>

<sota_updates>
## State of the Art (2025-2026)

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| unp4k only | scdatatools preferred | 2024-2025 | scdatatools has better CLI, JSON output, active maintenance |
| XML output | JSON output | Always available | JSON is 30-50% smaller, easier to parse |
| Manual extraction | Cached extraction | Best practice | Avoid re-extracting 150GB on every run |

**New tools/patterns to consider:**
- **scdatatools 1.0.4** (July 2025): Latest version, Python 3.10+
- **StarBreaker Avalonia UI**: GUI for exploration, useful for understanding data structure

**Deprecated/outdated:**
- **scunpacked (original)**: Archived, use StarCitizenWiki fork or scdatatools
- **Direct XML parsing**: JSON output from scdatatools is preferred
</sota_updates>

<open_questions>
## Open Questions

1. **Wikelo Contract Record Type**
   - What we know: Contracts exist in DataForge as some record type
   - What's unclear: Exact type name for Wikelo contracts (MissionContract? CollectionMission?)
   - Recommendation: Extract DataForge, search for "wikelo" in record names/content

2. **Item→Source Mapping Location**
   - What we know: Items and sources exist in DataForge
   - What's unclear: Whether item→source is explicit or inferred from spawn/loot tables
   - Recommendation: Explore extracted data structure before finalizing schema

3. **scdatatools vs 7z+unforge**
   - What we know: Both work; scdatatools is Python, unp4k is .NET
   - What's unclear: Which is faster, which handles edge cases better
   - Recommendation: Start with scdatatools (simpler setup); fall back to 7z if needed

4. **Cache Invalidation Strategy**
   - What we know: Need to re-extract when game updates
   - What's unclear: Best way to detect game version change
   - Recommendation: Hash p4k file metadata (size + mtime) or read game version from extracted data
</open_questions>

<sources>
## Sources

### Primary (HIGH confidence)
- [scdatatools documentation](https://scdatatools.readthedocs.io/en/latest/readme.html) — Python API reference
- [unp4k GitHub](https://github.com/dolkensp/unp4k) — P4k format details, encryption info
- [StarBreaker GitHub](https://github.com/diogotr7/StarBreaker) — DCB extraction commands

### Secondary (MEDIUM confidence)
- [scdatatools PyPI](https://pypi.org/project/scdatatools/) — Last updated July 2025
- [StarCitizenWiki scunpacked](https://github.com/StarCitizenWiki/scunpacked) — Forked, maintained

### Tertiary (LOW confidence - needs validation)
- None — all findings verified against official tool repositories
</sources>

<metadata>
## Metadata

**Research scope:**
- Core technology: Star Citizen Data.p4k, DataForge DCB
- Ecosystem: scdatatools (Python), unp4k (.NET), 7z
- Patterns: External tool abstraction, cached extraction, streaming JSON parse
- Pitfalls: Performance, tool availability, format versioning

**Confidence breakdown:**
- Standard stack: HIGH — verified against tool documentation
- Architecture: HIGH — follows established patterns for external tool integration
- Pitfalls: HIGH — common issues documented in tool repos
- Code examples: MEDIUM — patterns are standard, exact scdatatools CLI flags need verification

**Research date:** 2026-01-16
**Valid until:** 2026-02-16 (30 days — tools are stable but game updates may change formats)
</metadata>

---

*Phase: 02.1-game-data-extraction*
*Research completed: 2026-01-16*
*Ready for planning: yes*
